# Description
This directory contains my labs for deep learning course.

# Lab 1
Implements different activation and loss functions via numpy.

# Lab 2
Implements class 'Value', which is inspired by Amdrej Karpathy micrograd library. I implemented my version of gradient descent algorithm, and additionally implemented Adam optimizer.

# Lab 3
Implements MLP (multilayer perceptron) class with adjustable number of layers, neurons per each layer and activations.

# Lab 4
Adds batch normalization and droput to the MLP from Lab 3.

# Lab 5
Custom implementation of ResNet (residual network), specifically ResNet20. The number of layers and block per layer are adjustable. The model is trained on CIFAR10 image dataset.

# Lab 6
Custom implementation of Vision Transformer in combination with ResNet20 for feature extraction.
